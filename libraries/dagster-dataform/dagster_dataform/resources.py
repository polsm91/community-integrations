import dagster as dg
from typing import Any

from dagster_dataform.utils import get_epoch_time_ago, empty_fn

from google.cloud import dataform_v1


class DataformRepositoryResource:
    """This resource exposes methods for interacting with the Dataform resource via the GCP Python SDK."""

    def __init__(
        self,
        project_id: str,
        repository_id: str,
        location: str,
        environment: str,
        sensor_minimum_interval_seconds: int = 120,
        client: dataform_v1.DataformClient | None = None,
        asset_fresh_policy_lag_minutes: float = 1440,
        skip_compilation: bool = False,
    ):
        self.project_id = project_id
        self.location = location
        self.repository_id = repository_id
        self.environment = environment
        self.client = client if client is not None else dataform_v1.DataformClient()
        self.logger = dg.get_dagster_logger()
        self.sensor_minimum_interval_seconds = sensor_minimum_interval_seconds
        self.asset_fresh_policy_lag_minutes = asset_fresh_policy_lag_minutes
        self.skip_compilation = skip_compilation

        if skip_compilation:
            # Skip Dataform model compilation, use lazy loading
            self._assets = None
            self._asset_checks = None
        else:
            # Compile Dataform models
            self.assets = self.load_dataform_assets(
                fresh_policy_lag_minutes=self.asset_fresh_policy_lag_minutes
            )
            self.asset_checks = self.load_dataform_asset_check_specs()

    @property
    def assets(self) -> list[dg.AssetSpec]:
        """Lazy-load assets, fetching latest compilation when needed."""
        if self.skip_compilation and self._assets is None:
            self._assets = self._fetch_latest_compilation()
        if self._assets is None:
            return []
        return self._assets

    @assets.setter
    def assets(self, value: list[dg.AssetSpec]):
        """Setter for assets to support both lazy and immediate loading."""
        self._assets = value

    @property
    def asset_checks(self) -> list[dg.AssetChecksDefinition]:
        """Lazy-load asset checks, fetching latest compilation when needed."""
        if self.skip_compilation and self._asset_checks is None:
            compilation_result_name, compilation_actions = (
                self._fetch_latest_compilation_actions()
            )
            self._asset_checks = self._process_asset_checks(compilation_actions)
        if self._asset_checks is None:
            return []
        return self._asset_checks

    @asset_checks.setter
    def asset_checks(self, value: list[dg.AssetChecksDefinition]):
        """Setter for asset_checks to support both lazy and immediate loading."""
        self._asset_checks = value

    def _fetch_latest_compilation(self) -> list[dg.AssetSpec]:
        """Fetch the most recent compilation instead of creating new one."""
        compilation_result_name, compilation_actions = (
            self._fetch_latest_compilation_actions()
        )
        return self._process_assets(compilation_actions, compilation_result_name)

    def _fetch_latest_compilation_actions(self) -> tuple[str, list[Any]]:
        """Fetch compilation actions from the latest compilation result. Returns (compilation_result_name, actions)."""
        compilation_result_name = self.get_latest_compilation_result_name()
        if not compilation_result_name:
            error_msg = (
                f"No existing compilation result found for branch '{self.environment}'. "
                "Since 'skip_compilation=True', a valid compilation must already exist in GCP "
                "(created by CI/CD or an external process)."
            )
            self.logger.error(error_msg)
            raise Exception(error_msg)

        self.logger.info(f"Querying compilation result: {compilation_result_name}")

        request = dataform_v1.QueryCompilationResultActionsRequest(
            name=compilation_result_name,
        )

        response = self.client.query_compilation_result_actions(request=request)
        self.logger.info(
            f"Found {len(response.compilation_result_actions)} compilation result actions"
        )

        return compilation_result_name, response.compilation_result_actions

    def _process_assets(
        self, compilation_actions: list[Any], compilation_result_name: str | None = None
    ) -> list[dg.AssetSpec]:
        """Process compilation actions into asset specs."""
        logger = dg.get_dagster_logger()
        logger.info(
            f"Processing {len(compilation_actions)} compilation actions for assets"
        )

        assets = []
        for asset in compilation_actions:
            try:
                spec = dg.AssetSpec(
                    key=asset.target.name,
                    kinds={"bigquery"},
                    metadata={
                        "Project ID": asset.target.database,
                        "Dataset": asset.target.schema,
                        "Asset Name": asset.target.name,
                        "Docs Link": dg.MetadataValue.url(
                            f"https://cvsdigital.atlassian.net/wiki/spaces/EDMLABCCM/pages/4616946342/Case+Activities+Entity+Data+Stream#{asset.target.name}"
                        ),
                        "Asset SQL Code": dg.MetadataValue.md(
                            f"```sql\n{asset.relation.select_query}\n```"
                        ),
                        "dataform_compilation_result": compilation_result_name,
                    },
                    group_name=asset.target.schema,
                    tags={tag: "" for tag in asset.relation.tags},
                    deps=[target.name for target in asset.relation.dependency_targets],
                    legacy_freshness_policy=dg.LegacyFreshnessPolicy(
                        maximum_lag_minutes=self.asset_fresh_policy_lag_minutes
                    ),
                )
                assets.append(spec)
                logger.debug(f"Created asset spec for: {asset.target.name}")
            except Exception as e:
                logger.error(
                    f"Failed to create asset spec for {asset.target.name}: {str(e)}"
                )

        logger.info(f"Successfully created {len(assets)} assets")
        return assets

    def _process_asset_checks(
        self, compilation_actions: list[Any]
    ) -> list[dg.AssetChecksDefinition]:
        """Process compilation actions into asset check definitions."""
        logger = dg.get_dagster_logger()
        logger.info(
            f"Processing {len(compilation_actions)} compilation actions for asset checks"
        )

        asset_checks = []
        for asset in compilation_actions:
            if asset.assertion:
                try:
                    asset_key = asset.assertion.parent_action.name

                    # Convert string to AssetKey
                    asset_key_obj = dg.AssetKey(asset_key)

                    spec = dg.AssetCheckSpec(
                        asset=asset_key_obj,  # Use AssetKey object, not string
                        name=asset.target.name,
                    )

                    definition = dg.AssetChecksDefinition.create(
                        keys_by_input_name={
                            "asset_key": asset_key_obj
                        },  # Use AssetKey object
                        node_def=dg.OpDefinition(
                            name=asset.target.name,
                            compute_fn=empty_fn,  # We want to simply define the asset check specifications, not a computations for the check. These checks will not be computed on the Dagster side.
                        ),
                        check_specs_by_output_name={"spec": spec},
                        can_subset=False,
                    )

                    asset_checks.append(definition)
                    logger.debug(f"Created asset check spec for: {asset.target.name}")
                except Exception as e:
                    logger.error(
                        f"Failed to create asset check spec for {asset.target.name}: {str(e)}"
                    )

        logger.info(f"Successfully created {len(asset_checks)} asset check specs")
        return asset_checks

    def create_compilation_result(
        self,
        git_commitish: str,
        default_database: str | None = None,
        default_schema: str | None = None,
        default_location: str | None = None,
        assertion_schema: str | None = None,
        database_suffix: str | None = None,
        schema_suffix: str | None = None,
        table_prefix: str | None = None,
        builtin_assertion_name_prefix: str | None = None,
        vars: dict[str, Any] | None = None,
    ) -> dataform_v1.CompilationResult:
        """Create a compilation result and return the name."""
        compilation_result = dataform_v1.CompilationResult()
        compilation_result.git_commitish = git_commitish
        compilation_result.code_compilation_config = dataform_v1.CodeCompilationConfig(
            default_database=default_database,
            default_schema=default_schema,
            default_location=default_location,
            assertion_schema=assertion_schema,
            database_suffix=database_suffix,
            schema_suffix=schema_suffix,
            table_prefix=table_prefix,
            builtin_assertion_name_prefix=builtin_assertion_name_prefix,
            vars=vars,
        )

        request = dataform_v1.CreateCompilationResultRequest(
            parent=f"projects/{self.project_id}/locations/{self.location}/repositories/{self.repository_id}",
            compilation_result=compilation_result,
        )

        response = self.client.create_compilation_result(request=request)

        self.logger.info(f"Created compilation result: {response.name}")

        return response

    def get_latest_compilation_result_name(self) -> str | None:
        """Get the latest compilation result for the repository.
        https://cloud.google.com/python/docs/reference/dataform/latest/google.cloud.dataform_v1.types.ListCompilationResultsRequest
        """

        self.logger.info(
            f"Fetching compilation results for repository: {self.repository_id}"
        )

        request = dataform_v1.ListCompilationResultsRequest(
            parent=f"projects/{self.project_id}/locations/{self.location}/repositories/{self.repository_id}",
            page_size=1000,
            order_by="create_time desc",
        )

        response = self.client.list_compilation_results(request=request)

        self.logger.info(
            f"Found {len(response.compilation_results)} compilation results"
        )

        for compilation_result in response.compilation_results:
            if (
                compilation_result.git_commitish == self.environment
                and not compilation_result.code_compilation_config.table_prefix
            ):
                self.logger.info(
                    f"Found existing compilation result for branch '{self.environment}': {compilation_result.name}"
                )
                return compilation_result.name

        self.logger.error(
            f"No compilation result for {self.environment} branch in the last 10 compilation results"
        )
        return None

    def query_compilation_result(self) -> list[Any]:
        """Query a compilation result by ID. Returns the compilation result actions."""

        compilation_result_name = self.get_latest_compilation_result_name()
        if not compilation_result_name:
            self.logger.error("No compilation result name available")
            return []

        self.logger.info(f"Querying compilation result: {compilation_result_name}")

        # Initialize request argument(s)
        request = dataform_v1.QueryCompilationResultActionsRequest(
            name=compilation_result_name,
        )

        # Make the request
        response = self.client.query_compilation_result_actions(request=request)

        self.logger.info(
            f"Found {len(response.compilation_result_actions)} compilation result actions"
        )

        # Handle the response
        return response.compilation_result_actions

    # def create_workflow_invocation(self, repository_id: str, workflow_id: str) -> dict:
    #     """Create a workflow invocation."""
    #     pass

    def get_latest_workflow_invocations(
        self, minutes_ago: int
    ) -> dataform_v1.ListWorkflowInvocationsResponse:
        """Get the latest workflow invocation."""
        request = dataform_v1.ListWorkflowInvocationsRequest(
            parent=f"projects/{self.project_id}/locations/{self.location}/repositories/{self.repository_id}",
            page_size=1000,
            filter=f"invocation_timing.start_time.seconds > {get_epoch_time_ago(minutes=minutes_ago)}",
        )

        response = self.client.list_workflow_invocations(request=request)

        self.logger.info(f"Found response: {response}")

        return response  # pyright: ignore[reportReturnType]

    def query_workflow_invocation(
        self, name: str
    ) -> dataform_v1.QueryWorkflowInvocationActionsResponse:
        """Query a workflow invocation by name."""

        if not name:
            self.logger.error("No workflow invocation name available")
            return []  # pyright: ignore[reportReturnType]

        self.logger.info(f"Querying workflow invocation: {name}")

        # Initialize request argument(s)
        request = dataform_v1.QueryWorkflowInvocationActionsRequest(
            name=name,
        )

        # Make the request
        response = self.client.query_workflow_invocation_actions(request=request)

        # self.logger.info(f"Found {len(response)} workflow invocation actions")

        # Handle the response
        return response  # pyright: ignore[reportReturnType]

    def create_workflow_invocation(
        self,
        compilation_result_name: str,
        included_targets: list[str | dict] | None = None,
        included_tags: list[str] | None = None,
        transitive_dependencies_included: bool = True,
        transitive_dependents_included: bool = False,
        fully_refresh_incremental_tables_enabled: bool = False,
        service_account: str | None = None,
    ) -> dataform_v1.WorkflowInvocation:
        """Create a workflow invocation. Returns the workflow invocation object.

        Args:
            compilation_result_name: The compilation result to use for the invocation.
            included_targets: List of targets to include (selective execution).
                Each target can be either:
                - A string (just the name, database/schema will be empty)
                - A dict with keys: database, schema, name
            included_tags: List of tags to filter targets by.
            transitive_dependencies_included: Include upstream dependencies of selected targets.
            transitive_dependents_included: Include downstream dependents of selected targets.
            fully_refresh_incremental_tables_enabled: Force full refresh of incremental tables.
            service_account: Service account email to run the workflow as. If not set,
                uses the repository's default service account (requires impersonation permission).
        """
        workflow_invocation = dataform_v1.WorkflowInvocation(
            compilation_result=compilation_result_name,
        )

        # Build invocation config if any options are specified
        if included_targets or included_tags or service_account:
            invocation_config = dataform_v1.InvocationConfig(
                transitive_dependencies_included=transitive_dependencies_included,
                transitive_dependents_included=transitive_dependents_included,
                fully_refresh_incremental_tables_enabled=fully_refresh_incremental_tables_enabled,
            )
            if included_targets:
                targets = []
                for target in included_targets:
                    if isinstance(target, dict):
                        # Only pass non-None values to avoid protobuf serialization issues
                        target_kwargs = {}
                        if target.get("database"):
                            target_kwargs["database"] = target["database"]
                        if target.get("schema"):
                            target_kwargs["schema"] = target["schema"]
                        if target.get("name"):
                            target_kwargs["name"] = target["name"]
                        targets.append(dataform_v1.Target(**target_kwargs))
                    else:
                        targets.append(dataform_v1.Target(name=target))
                invocation_config.included_targets = targets
            if included_tags:
                invocation_config.included_tags = included_tags
            if service_account:
                invocation_config.service_account = service_account
            workflow_invocation.invocation_config = invocation_config

        request = dataform_v1.CreateWorkflowInvocationRequest(
            parent=f"projects/{self.project_id}/locations/{self.location}/repositories/{self.repository_id}",
            workflow_invocation=workflow_invocation,
        )

        response = self.client.create_workflow_invocation(request=request)

        self.logger.info(f"Created workflow invocation: {response.name}")

        return response

    def get_workflow_invocation_details(
        self, workflow_invocation_name: str
    ) -> dataform_v1.WorkflowInvocation:
        """Get the details of a workflow invocation. Returns the workflow invocation object."""

        request = dataform_v1.GetWorkflowInvocationRequest(
            name=workflow_invocation_name,
        )

        response = self.client.get_workflow_invocation(request=request)

        return response

    def load_dataform_assets(
        self,
        fresh_policy_lag_minutes: float = 1440,
    ) -> list[dg.AssetSpec]:
        logger = dg.get_dagster_logger()
        logger.info("Starting to load Dataform assets")

        assets = []
        self.create_compilation_result(git_commitish=self.environment)
        compilation_actions = self.query_compilation_result()

        logger.info(f"Processing {len(compilation_actions)} compilation actions")

        for asset in compilation_actions:
            try:
                spec = dg.AssetSpec(
                    key=asset.target.name,
                    kinds={"bigquery"},
                    metadata={
                        "Project ID": asset.target.database,
                        "Dataset": asset.target.schema,
                        "Asset Name": asset.target.name,
                        "Docs Link": dg.MetadataValue.url(
                            f"https://cvsdigital.atlassian.net/wiki/spaces/EDMLABCCM/pages/4616946342/Case+Activities+Entity+Data+Stream#{asset.target.name}"
                        ),
                        # "github link": MetadataValue.url(f"https://github.com/cvs-health-source-code/hcm-cm-de-clinical-analytics-nexus-dataform/blob/{client.environment}/definitions/{asset.target.schema.split('_')[4]}/{asset.target.name}.sqlx")
                        "Asset SQL Code": dg.MetadataValue.md(
                            f"```sql\n{asset.relation.select_query}\n```"
                        ),
                    },
                    group_name=asset.target.schema,
                    tags={tag: "" for tag in asset.relation.tags},
                    deps=[target.name for target in asset.relation.dependency_targets],
                    legacy_freshness_policy=dg.LegacyFreshnessPolicy(
                        maximum_lag_minutes=fresh_policy_lag_minutes
                    ),
                )
                assets.append(spec)
                logger.debug(f"Created asset spec for: {asset.target.name}")
            except Exception as e:
                logger.error(
                    f"Failed to create asset spec for {asset.target.name}: {str(e)}"
                )

        logger.info(f"Successfully created {len(assets)} assets")
        return assets

    def load_dataform_asset_check_specs(
        self,
    ) -> list[dg.AssetChecksDefinition]:
        logger = dg.get_dagster_logger()
        logger.info("Starting to load Dataform asset check specs")

        asset_checks = []
        self.create_compilation_result(git_commitish=self.environment)
        compilation_actions = self.query_compilation_result()

        logger.info(f"Processing {len(compilation_actions)} compilation actions")

        for asset in compilation_actions:
            if asset.assertion:
                try:
                    logger.error(asset.assertion.dependency_targets[0].name)
                    asset_key = asset.assertion.parent_action.name

                    # Convert string to AssetKey
                    asset_key_obj = dg.AssetKey(asset_key)

                    spec = dg.AssetCheckSpec(
                        asset=asset_key_obj,  # Use AssetKey object, not string
                        name=asset.target.name,
                    )

                    definition = dg.AssetChecksDefinition.create(
                        keys_by_input_name={
                            "asset_key": asset_key_obj
                        },  # Use AssetKey object
                        node_def=dg.OpDefinition(
                            name=asset.target.name,
                            compute_fn=empty_fn,  # We want to simply define the asset check specifications, not a computations for the check. These checks will not be computed on the Dagster side.
                        ),
                        check_specs_by_output_name={"spec": spec},
                        can_subset=False,
                    )

                    asset_checks.append(definition)
                    logger.debug(f"Created asset check spec for: {asset.target.name}")
                except Exception as e:
                    logger.error(
                        f"Failed to create asset check spec for {asset.target.name}: {str(e)}"
                    )

        logger.error(f"Successfully created {len(asset_checks)} asset check specs")
        return asset_checks
